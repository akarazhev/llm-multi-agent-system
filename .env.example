# =============================================================================
# LLM Multi-Agent System - Environment Configuration
# =============================================================================

# -----------------------------------------------------------------------------
# Local LLM Server Configuration (Required)
# -----------------------------------------------------------------------------
# Base URL for the local llama-server API (OpenAI-compatible)
OPENAI_API_BASE=http://127.0.0.1:8080/v1

# API key (not needed for local server, but required by OpenAI client)
OPENAI_API_KEY=not-needed

# Model name/alias configured in your llama-server
# Example: devstral, llama3, mistral, etc.
OPENAI_API_MODEL=devstral

# LLM Generation Parameters
OPENAI_TEMPERATURE=0.7

# Maximum tokens for LLM response generation
# IMPORTANT: This must be less than your llama-server --ctx-size
# Your llama-server: --ctx-size 16384 --batch-size 512
# 
# Recommended values based on context window:
#   - ctx-size 16384: Use 8192 (leaves 8K for prompt)
#   - ctx-size 32768: Use 16384 (leaves 16K for prompt)
#   - ctx-size 65536: Use 32768 (leaves 32K for prompt)
# 
# Agent-specific recommendations:
#   - Business Analyst (comprehensive docs): 8192-16384
#   - Technical Writer (large docs): 8192-16384
#   - Developer (code generation): 4096-8192
#   - QA Engineer (test cases): 4096
#   - DevOps Engineer (configs): 4096
# 
# NOTE: If responses are truncated, increase this value!
OPENAI_MAX_TOKENS=8192

# -----------------------------------------------------------------------------
# Retry and Resilience Configuration
# -----------------------------------------------------------------------------
# Maximum number of retry attempts for failed LLM calls
LLM_MAX_RETRIES=3

# Initial delay between retries (seconds)
LLM_RETRY_INITIAL_DELAY=1.0

# Maximum delay between retries (seconds)
LLM_RETRY_MAX_DELAY=60.0

# Circuit Breaker Configuration
# Number of failures before circuit breaker opens
LLM_CIRCUIT_BREAKER_THRESHOLD=5

# Time to wait before attempting recovery (seconds)
LLM_CIRCUIT_BREAKER_TIMEOUT=60.0

# Number of successful calls needed to close circuit breaker
LLM_CIRCUIT_BREAKER_HALF_OPEN=3

# -----------------------------------------------------------------------------
# Logging Configuration
# -----------------------------------------------------------------------------
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Enable JSON-formatted structured logging (recommended for production)
STRUCTURED_LOGGING=true

# Enable colored console output (useful for development)
COLORED_CONSOLE=true

# Log file path (optional)
LOG_FILE=logs/agent_system.log

# -----------------------------------------------------------------------------
# Workspace and Output Configuration
# -----------------------------------------------------------------------------
# Project workspace directory
WORKSPACE=.

# Output directory for generated files
OUTPUT_DIRECTORY=./output

# -----------------------------------------------------------------------------
# Configuration File
# -----------------------------------------------------------------------------
# Path to the YAML configuration file
AGENT_CONFIG_PATH=config.yaml

# -----------------------------------------------------------------------------
# Performance and Resource Management
# -----------------------------------------------------------------------------
# Maximum number of concurrent agent tasks
MAX_CONCURRENT_AGENTS=5

# Task timeout in seconds
TASK_TIMEOUT=600

# LLM API timeout in seconds
LLM_TIMEOUT=300

# -----------------------------------------------------------------------------
# Feature Flags
# -----------------------------------------------------------------------------
# Enable metrics collection and monitoring
ENABLE_METRICS=true

# Enable structured logging
ENABLE_STRUCTURED_LOGGING=true

# Enable task persistence (experimental)
ENABLE_TASK_PERSISTENCE=false

# Enable inter-agent message bus
ENABLE_MESSAGE_BUS=true

# Enable response streaming by default
LLM_STREAM_RESPONSES=true

# -----------------------------------------------------------------------------
# Development/Debug Settings
# -----------------------------------------------------------------------------
# Enable debug mode (verbose logging)
DEBUG=false

# Enable profiling (performance analysis)
ENABLE_PROFILING=false

# -----------------------------------------------------------------------------
# Integration Configuration (Optional)
# -----------------------------------------------------------------------------
# JIRA Configuration
# JIRA_URL=https://your-company.atlassian.net
# JIRA_EMAIL=your-email@company.com
# JIRA_API_TOKEN=your-api-token

# GitLab Configuration
# GITLAB_URL=https://gitlab.com
# GITLAB_TOKEN=your-gitlab-token
# GITLAB_PROJECT_ID=your-project-id

# Slack Notifications (Optional)
# SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL

# -----------------------------------------------------------------------------
# Security Configuration
# -----------------------------------------------------------------------------
# Secret management (if using external secrets)
# VAULT_ADDR=https://vault.example.com
# VAULT_TOKEN=your-vault-token

# -----------------------------------------------------------------------------
# Monitoring and Observability (Optional)
# -----------------------------------------------------------------------------
# Prometheus metrics endpoint port
# METRICS_PORT=9090

# Health check endpoint port
# HEALTH_CHECK_PORT=8000

# Sentry DSN for error tracking (optional)
# SENTRY_DSN=https://your-sentry-dsn

# Datadog API key (optional)
# DATADOG_API_KEY=your-datadog-api-key

# -----------------------------------------------------------------------------
# Troubleshooting
# -----------------------------------------------------------------------------
# If you experience truncated responses:
#   1. Increase OPENAI_MAX_TOKENS (see recommendations above)
#   2. Check llama-server context size: ps aux | grep llama-server
#   3. Monitor token usage in logs
#   4. See docs/LLM_TOKEN_LIMITS.md for detailed guidance
#
# If you experience timeouts:
#   1. Increase LLM_TIMEOUT
#   2. Check llama-server performance (CPU/GPU usage)
#   3. Reduce OPENAI_MAX_TOKENS if server is slow
#
# If circuit breaker opens frequently:
#   1. Increase LLM_CIRCUIT_BREAKER_THRESHOLD
#   2. Increase LLM_CIRCUIT_BREAKER_TIMEOUT
#   3. Check llama-server health and logs

# -----------------------------------------------------------------------------
# Notes
# -----------------------------------------------------------------------------
# 1. Copy this file to .env and update with your actual values
# 2. Never commit .env to version control
# 3. The local llama-server must be running before starting the agent system
# 4. Adjust retry and timeout values based on your LLM server performance
# 5. Enable structured logging in production for better log aggregation
# 6. Monitor circuit breaker metrics to tune threshold values
# 7. For comprehensive documentation tasks, use OPENAI_MAX_TOKENS=8192 or higher
