# =============================================================================
# LLM Multi-Agent System - Environment Configuration
# =============================================================================

# -----------------------------------------------------------------------------
# Local LLM Server Configuration (Required)
# -----------------------------------------------------------------------------
# Base URL for the local llama-server API (OpenAI-compatible)
OPENAI_API_BASE=http://127.0.0.1:8080/v1

# API key (not needed for local server, but required by OpenAI client)
OPENAI_API_KEY=not-needed

# Model name/alias configured in your llama-server
# Example: devstral, llama3, mistral, etc.
OPENAI_API_MODEL=devstral

# LLM Generation Parameters
OPENAI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=2048

# -----------------------------------------------------------------------------
# Retry and Resilience Configuration
# -----------------------------------------------------------------------------
# Maximum number of retry attempts for failed LLM calls
LLM_MAX_RETRIES=3

# Initial delay between retries (seconds)
LLM_RETRY_INITIAL_DELAY=1.0

# Maximum delay between retries (seconds)
LLM_RETRY_MAX_DELAY=60.0

# Circuit Breaker Configuration
# Number of failures before circuit breaker opens
LLM_CIRCUIT_BREAKER_THRESHOLD=5

# Time to wait before attempting recovery (seconds)
LLM_CIRCUIT_BREAKER_TIMEOUT=60.0

# Number of successful calls needed to close circuit breaker
LLM_CIRCUIT_BREAKER_HALF_OPEN=3

# -----------------------------------------------------------------------------
# Logging Configuration
# -----------------------------------------------------------------------------
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Enable JSON-formatted structured logging (recommended for production)
STRUCTURED_LOGGING=true

# Enable colored console output (useful for development)
COLORED_CONSOLE=true

# Log file path (optional)
LOG_FILE=logs/agent_system.log

# -----------------------------------------------------------------------------
# Workspace and Output Configuration
# -----------------------------------------------------------------------------
# Project workspace directory
WORKSPACE=.

# Output directory for generated files
OUTPUT_DIRECTORY=./output

# -----------------------------------------------------------------------------
# Configuration File
# -----------------------------------------------------------------------------
# Path to the YAML configuration file
AGENT_CONFIG_PATH=config.yaml

# -----------------------------------------------------------------------------
# Performance and Resource Management
# -----------------------------------------------------------------------------
# Maximum number of concurrent agent tasks
MAX_CONCURRENT_AGENTS=5

# Task timeout in seconds
TASK_TIMEOUT=600

# LLM API timeout in seconds
LLM_TIMEOUT=300

# -----------------------------------------------------------------------------
# Feature Flags
# -----------------------------------------------------------------------------
# Enable metrics collection and monitoring
ENABLE_METRICS=true

# Enable structured logging
ENABLE_STRUCTURED_LOGGING=true

# Enable task persistence (experimental)
ENABLE_TASK_PERSISTENCE=false

# Enable inter-agent message bus
ENABLE_MESSAGE_BUS=true

# Enable response streaming by default
LLM_STREAM_RESPONSES=true

# -----------------------------------------------------------------------------
# Development/Debug Settings
# -----------------------------------------------------------------------------
# Enable debug mode (verbose logging)
DEBUG=false

# Enable profiling (performance analysis)
ENABLE_PROFILING=false

# -----------------------------------------------------------------------------
# Integration Configuration (Optional)
# -----------------------------------------------------------------------------
# JIRA Configuration
# JIRA_URL=https://your-company.atlassian.net
# JIRA_EMAIL=your-email@company.com
# JIRA_API_TOKEN=your-api-token

# GitLab Configuration
# GITLAB_URL=https://gitlab.com
# GITLAB_TOKEN=your-gitlab-token
# GITLAB_PROJECT_ID=your-project-id

# Slack Notifications (Optional)
# SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL

# -----------------------------------------------------------------------------
# Security Configuration
# -----------------------------------------------------------------------------
# Secret management (if using external secrets)
# VAULT_ADDR=https://vault.example.com
# VAULT_TOKEN=your-vault-token

# -----------------------------------------------------------------------------
# Monitoring and Observability (Optional)
# -----------------------------------------------------------------------------
# Prometheus metrics endpoint port
# METRICS_PORT=9090

# Health check endpoint port
# HEALTH_CHECK_PORT=8000

# Sentry DSN for error tracking (optional)
# SENTRY_DSN=https://your-sentry-dsn

# Datadog API key (optional)
# DATADOG_API_KEY=your-datadog-api-key

# -----------------------------------------------------------------------------
# Notes
# -----------------------------------------------------------------------------
# 1. Copy this file to .env and update with your actual values
# 2. Never commit .env to version control
# 3. The local llama-server must be running before starting the agent system
# 4. Adjust retry and timeout values based on your LLM server performance
# 5. Enable structured logging in production for better log aggregation
# 6. Monitor circuit breaker metrics to tune threshold values
