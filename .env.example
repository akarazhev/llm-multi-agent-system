# ============================================================================
# LLM Multi-Agent System - Environment Configuration
# ============================================================================

# ============================================================================
# Local LLM Server Configuration (REQUIRED)
# ============================================================================

# Base URL for local llama-server (OpenAI-compatible API)
# Default: http://127.0.0.1:8080/v1
# This MUST point to your running llama-server instance
OPENAI_API_BASE=http://127.0.0.1:8080/v1

# API Key (not used for local server, but required by OpenAI client)
# Can be any value - local server doesn't validate it
OPENAI_API_KEY=not-needed

# Model name to use
# Must match the model loaded in llama-server
# Common options: devstral, qwen2.5-coder, deepseek-coder, codellama
OPENAI_API_MODEL=devstral

# ============================================================================
# Workspace Configuration (Optional)
# ============================================================================

# Path to workspace directory
# If not set, uses cursor_workspace from config.yaml
# Can be absolute or relative path
# CURSOR_WORKSPACE=.
# CURSOR_WORKSPACE=/absolute/path/to/workspace

# ============================================================================
# Configuration File Path (Optional)
# ============================================================================

# Path to YAML configuration file
# Default: config.yaml
# AGENT_CONFIG_PATH=config.yaml
# AGENT_CONFIG_PATH=/path/to/custom/config.yaml

# ============================================================================
# Logging Configuration (Optional)
# ============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Default: INFO (from config.yaml)
# LOG_LEVEL=INFO

# Log file path
# Default: logs/agent_system.log (from config.yaml)
# LOG_FILE=logs/agent_system.log

# ============================================================================
# LLM Server Advanced Configuration (Optional)
# ============================================================================

# Connection timeout for LLM API calls (seconds)
# Default: 300 (from config.yaml)
# LLM_TIMEOUT=300

# Maximum retries for failed API calls
# Default: 3
# LLM_MAX_RETRIES=3

# ============================================================================
# llama.cpp Configuration (For script usage)
# ============================================================================

# Model to download/use with llama-server
# Model configuration for llama-server (if using)
# LLAMA_MODEL=unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF:UD-Q4_K_XL

# Number of GPU layers to offload
# 99 = offload all layers (recommended for GPU)
# 0 = CPU only
# LLAMA_GPU_LAYERS=99

# Context size (number of tokens)
# Larger = more context but slower and more memory
# Typical values: 4096, 8192, 16384, 32768
# LLAMA_CTX_SIZE=16384

# Number of CPU threads to use
# Default: auto-detect
# LLAMA_THREADS=8

# llama-server host binding
# 127.0.0.1 = localhost only (recommended for security)
# 0.0.0.0 = all interfaces (only if you need network access)
# LLAMA_HOST=127.0.0.1

# llama-server port
# Default: 8080
# LLAMA_PORT=8080

# ============================================================================
# Integration Configuration (Future Features)
# ============================================================================

# Jira Configuration (Future)
# JIRA_URL=https://your-domain.atlassian.net
# JIRA_EMAIL=your-email@example.com
# JIRA_API_TOKEN=your-api-token

# Confluence Configuration (Future)
# CONFLUENCE_URL=https://your-domain.atlassian.net
# CONFLUENCE_EMAIL=your-email@example.com
# CONFLUENCE_API_TOKEN=your-api-token

# GitLab Configuration (Future)
# GITLAB_URL=https://gitlab.com
# GITLAB_TOKEN=your-gitlab-token

# GitHub Configuration (Future)
# GITHUB_TOKEN=your-github-token

# ============================================================================
# Performance Tuning (Optional)
# ============================================================================

# Maximum concurrent agents
# Default: 5 (from config.yaml)
# MAX_CONCURRENT_AGENTS=5

# Task timeout (seconds)
# Default: 600 (from config.yaml)
# TASK_TIMEOUT=600

# Task retry attempts
# Default: 3 (from config.yaml)
# TASK_RETRY_ATTEMPTS=3

# ============================================================================
# Development & Debug (Optional)
# ============================================================================

# Enable debug mode
# Set to "true" to enable verbose logging
# DEBUG=false

# Enable development mode features
# DEV_MODE=false

# Python path additions (if needed)
# PYTHONPATH=/additional/python/path

# ============================================================================
# Notes
# ============================================================================
#
# 1. REQUIRED Settings:
#    - OPENAI_API_BASE: Must point to running llama-server
#    - OPENAI_API_KEY: Can be any value (not validated locally)
#    - OPENAI_API_MODEL: Must match loaded model
#
# 2. Privacy & Security:
#    - All data stays local - no external API calls
#    - llama-server should bind to 127.0.0.1 (localhost only)
#    - No real API keys needed
#
# 3. Model Selection:
#    - Devstral-Small-2-24B (default): Fast, good for coding
#    - Qwen2.5-Coder-32B: Excellent code generation
#    - DeepSeek-Coder-33B: Strong coding capabilities
#    - CodeLlama-34B: Meta's code model
#
# 4. Performance:
#    - Use GPU acceleration when available (LLAMA_GPU_LAYERS=99)
#    - Larger context size = more memory usage
#    - Adjust LLAMA_THREADS based on CPU cores
#
# 5. Troubleshooting:
#    - If "Connection refused": Ensure your local LLM server is running on port 8080
#    - If "Model not found": Check OPENAI_API_MODEL matches loaded model
#    - If slow: Increase LLAMA_GPU_LAYERS or use smaller model
#    - If OOM: Reduce LLAMA_CTX_SIZE or use smaller quantization
#
# For more information, see:
# - docs/QUICK_START.md
# - docs/LOCAL_ONLY_MODE.md
# - docs/TROUBLESHOOTING.md
#
# ============================================================================
