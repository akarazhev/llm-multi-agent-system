AGENT_CONFIG_PATH=config.yaml

# Use "." for current directory (project root)
CURSOR_WORKSPACE=.

LOG_LEVEL=INFO
LOG_FILE=logs/agent_system.log

OUTPUT_DIRECTORY=./output

# ============================================================================
# LOCAL LLAMA.CPP SERVER CONFIGURATION (Required)
# ============================================================================
# This system is configured to use ONLY local llama.cpp server
# No cloud APIs (OpenAI/Anthropic) are used

# Local llama-server endpoint (OpenAI-compatible API)
OPENAI_API_BASE=http://127.0.0.1:8080/v1
OPENAI_API_KEY=not-needed
OPENAI_API_MODEL=devstral

# Start the server with: ./scripts/start_llama_server.sh
# Check status with: ./scripts/check_llama_server.sh

# ============================================================================
# OPTIONAL: Customize llama-server settings
# ============================================================================
# These can be set before starting the server:
# export LLAMA_MODEL="unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF:UD-Q4_K_XL"
# export LLAMA_PORT=8080
# export LLAMA_CTX_SIZE=16384
# export LLAMA_GPU_LAYERS=99
